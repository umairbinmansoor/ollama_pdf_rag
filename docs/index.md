# Ollama PDF RAG Documentation

Welcome to the documentation for Ollama PDF RAG, a powerful local RAG (Retrieval Augmented Generation) application that lets you chat with your PDF documents using Ollama and LangChain.

## Overview

This project provides both a Streamlit web interface and a Jupyter notebook for experimenting with PDF-based question answering using local language models. All processing happens locally on your machine, ensuring privacy and data security.

## Key Features

- 🔒 **Fully Local Processing**: No data leaves your machine
- 📄 **PDF Processing**: Intelligent chunking and processing of PDF documents
- 🧠 **Multi-query Retrieval**: Better context understanding through multiple query generation
- 🎯 **Advanced RAG**: Sophisticated implementation using LangChain
- 🖥️ **Clean Interface**: User-friendly Streamlit interface
- 📓 **Experimentation**: Jupyter notebook for testing and development

## Quick Links

- [Installation Guide](getting-started/installation.md)
- [Quick Start Tutorial](getting-started/quickstart.md)
- [User Guide](user-guide/pdf-processing.md)
- [API Reference](api/document.md)
- [Contributing Guide](development/contributing.md)

## Project Status

[![Python Tests](https://github.com/tonykipkemboi/ollama_pdf_rag/actions/workflows/tests.yml/badge.svg)](https://github.com/tonykipkemboi/ollama_pdf_rag/actions/workflows/tests.yml)

## Support

If you need help or want to contribute:

- 🐛 [Report a bug](https://github.com/tonykipkemboi/ollama_pdf_rag/issues)
- 💡 [Request a feature](https://github.com/tonykipkemboi/ollama_pdf_rag/issues)
- 🤝 [Contribute to the project](development/contributing.md)

## License

This project is open source and available under the MIT License. 